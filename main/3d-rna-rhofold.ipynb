{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":87793,"databundleVersionId":12276181,"sourceType":"competition"},{"sourceId":11974777,"sourceType":"datasetVersion","datasetId":7530539},{"sourceId":11989235,"sourceType":"datasetVersion","datasetId":7540626},{"sourceId":414684,"sourceType":"modelInstanceVersion","modelInstanceId":337526,"modelId":358495}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import subprocess\nimport sys\nimport glob\n\nwheel_files = glob.glob(\"/kaggle/input/rhofold-dependencies-for-offline-use/*.whl\")\nwheel_files = [w for w in wheel_files if 'numpy' not in w.lower()] # skip numpy to avoid compatibility issues\n\nif wheel_files:\n    subprocess.check_call([\n        sys.executable, \n        \"-m\", \n        \"pip\", \n        \"install\", \n        \"--no-index\",\n        \"--find-links\",\n        \"/kaggle/input/rhofold-dependencies-for-offline-use/\"\n    ] + wheel_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:01:03.424655Z","iopub.execute_input":"2025-05-29T00:01:03.424856Z","iopub.status.idle":"2025-05-29T00:01:09.058162Z","shell.execute_reply.started":"2025-05-29T00:01:03.424841Z","shell.execute_reply":"2025-05-29T00:01:09.057506Z"}},"outputs":[{"name":"stdout","text":"Looking in links: /kaggle/input/rhofold-dependencies-for-offline-use/\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/OpenMM-8.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/simtk-0.1.0-py2.py3-none-any.whl\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/biopython-1.85-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/ml_collections-1.1.0-py3-none-any.whl\nProcessing /kaggle/input/rhofold-dependencies-for-offline-use/absl_py-2.3.0-py3-none-any.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from OpenMM==8.2.0) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->OpenMM==8.2.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->OpenMM==8.2.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->OpenMM==8.2.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->OpenMM==8.2.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->OpenMM==8.2.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->OpenMM==8.2.0) (2024.2.0)\nPyYAML is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nInstalling collected packages: simtk, absl-py, ml-collections, OpenMM, biopython\n  Attempting uninstall: absl-py\n    Found existing installation: absl-py 1.4.0\n    Uninstalling absl-py-1.4.0:\n      Successfully uninstalled absl-py-1.4.0\nSuccessfully installed OpenMM-8.2.0 absl-py-2.3.0 biopython-1.85 ml-collections-1.1.0 simtk-0.1.0\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ntensorflow-metadata 1.17.0 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\nimport tempfile\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom pathlib import Path\nfrom scipy.spatial.distance import cdist\nfrom scipy.optimize import minimize\nfrom torch.utils.data import Dataset, DataLoader\n\nsys.path.append('/kaggle/input/rhofold/pytorch/default/2/RhoFold-main')\nfrom rhofold.config import rhofold_config\nfrom rhofold.relax.relax import AmberRelaxation\nfrom rhofold.rhofold import RhoFold\nfrom rhofold.utils import get_device, save_ss2ct, timing\nfrom rhofold.utils.alphabet import get_features\nfrom rhofold.rhofold import RhoFold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T00:01:36.044191Z","iopub.execute_input":"2025-05-29T00:01:36.044464Z","iopub.status.idle":"2025-05-29T00:01:36.110994Z","shell.execute_reply.started":"2025-05-29T00:01:36.044446Z","shell.execute_reply":"2025-05-29T00:01:36.110511Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_seq = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_sequences.csv\")\ntrain_seq_v2 = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_sequences.v2.csv\")\ntrain_labels = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_labels.csv\")\ntrain_labels_v2 = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/train_labels.v2.csv\")\nval_seq = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/validation_sequences.csv\")\nval_labels = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/validation_labels.csv\")\ntest_seq = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/test_sequences.csv\")\nsample_submission = pl.read_csv(\"/kaggle/input/stanford-rna-3d-folding/sample_submission.csv\")\n\n# dfs = [\n#     (train_seq, \"train_seq_head.csv\"),\n#     (train_seq_v2, \"train_seq_v2_head.csv\"),\n#     (train_labels, \"train_labels_head.csv\"),\n#     (train_labels_v2, \"train_labels_v2_head.csv\"),\n#     (val_seq, \"val_seq_head.csv\"),\n#     (val_labels, \"val_labels_head.csv\"),\n#     (test_seq, \"test_seq_head.csv\"),\n#     (sample_submission, \"sample_submission_head.csv\")\n# ]\n\n# for df in dfs:\n#     df[0].head().write_csv(df[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T02:16:00.566714Z","iopub.execute_input":"2025-05-29T02:16:00.566934Z","iopub.status.idle":"2025-05-29T02:16:01.530242Z","shell.execute_reply.started":"2025-05-29T02:16:00.566920Z","shell.execute_reply":"2025-05-29T02:16:01.529641Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class ReactivityDataset(Dataset):\n    def __init__(self, csv_path, max_len=206):\n        # RhoFold token mapping\n        self.base_to_token = {\n            'A': 0, 'C': 1, 'G': 2, 'U': 3, 'T': 3,  # T->U\n            'R': 4, 'Y': 5, 'K': 6, 'M': 7, 'S': 8, 'W': 9,\n            'B': 10, 'D': 11, 'H': 12, 'V': 13, 'N': 14, '-': 15\n        }\n        self.pad_token = 15  # Using '-' as pad\n        self.df = pl.read_csv(csv_path)\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.row(idx, named=True)\n        seq = row['sequence'].upper().replace('T', 'U')\n        \n        tokens = [self.base_to_token.get(base, 14) for base in seq]  # N for unknown\n        tokens = tokens[:self.max_len]\n        tokens += [self.pad_token] * (self.max_len - len(tokens))\n        \n        reactivity = []\n        mask = []\n        \n        for i in range(1, self.max_len + 1):\n            val = row[f'reactivity_{i:04d}']\n            \n            if i <= len(seq) and val not in [None, 'NULL'] and not (isinstance(val, float) and pl.Series([val]).is_nan()[0]):\n                reactivity.append(float(val))\n                mask.append(True)\n            else:\n                reactivity.append(0.0)\n                mask.append(False)\n        \n        return {\n            'tokens': torch.LongTensor(tokens),\n            'reactivity': torch.FloatTensor(reactivity),\n            'mask': torch.BoolTensor(mask),\n            'is_dms': row['experiment_type'] == 'DMS_MaP',\n            'seq_len': len(seq)\n        }\n\ndef collate_fn(batch):\n    tokens = torch.stack([x['tokens'] for x in batch])\n    mask = torch.stack([x['mask'] for x in batch])\n    \n    dms = torch.zeros_like(tokens, dtype=torch.float)\n    shape = torch.zeros_like(tokens, dtype=torch.float)\n    \n    for i, item in enumerate(batch):\n        if item['is_dms']:\n            dms[i] = item['reactivity']\n        else:\n            shape[i] = item['reactivity']\n    \n    return {\n        'tokens': tokens,\n        'dms': dms,\n        'shape': shape,\n        'mask': mask\n    }\n\nreactivity_dataset = ReactivityDataset('/kaggle/input/stanford-ribonanza-training-data/reactivity_train_data_sn_filtered.csv')\nreactivity_dataloader = DataLoader(reactivity_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualReactivityModel(nn.Module):\n    \"\"\"Stage 1: Pretrain RNA-FM on chemical reactivity data\"\"\"\n    def __init__(self, rna_fm, freeze_early_layers=True):\n        super().__init__()\n        self.rna_fm = rna_fm\n        \n        if freeze_early_layers:\n            for i, layer in enumerate(self.rna_fm.layers):\n                if i < 6:  # Freeze first 6/12 layers\n                    for param in layer.parameters():\n                        param.requires_grad = False\n        \n        # Shared projection\n        self.shared_proj = nn.Sequential(\n            nn.Linear(640, 320),\n            nn.ReLU(),\n            nn.LayerNorm(320)\n        )\n        \n        # Task-specific heads\n        self.dms_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n        self.shape_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n    def forward(self, tokens):\n        # Get RNA-FM representations\n        rna_out = self.rna_fm(tokens, repr_layers=[12])\n        features = rna_out['representations'][12]\n        \n        # Shared features\n        shared = self.shared_proj(features)\n        \n        # Predictions\n        dms = self.dms_head(shared).squeeze(-1)\n        shape = self.shape_head(shared).squeeze(-1)\n        \n        return {\n            'dms': dms,\n            'shape': shape,\n            'features': features\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = RhoFold(rhofold_config)\ncheckpoint = torch.load(\"/kaggle/input/rhofold/pytorch/default/2/RhoFold-main/pretrained/rhofold_pretrained_params.pt\", map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint['model'])\n\n# ...\n\n\ndef train_reactivity_stage(model, dataloader, epochs=10):\n    \"\"\"Stage 1: Train on reactivity data\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    for epoch in range(epochs):\n        for batch in dataloader:\n            tokens = batch['tokens']\n            dms_true = batch['dms']\n            shape_true = batch['shape']\n            \n            outputs = model(tokens)\n            \n            # Multi-task loss\n            loss = (F.mse_loss(outputs['dms'], dms_true) + \n                   F.mse_loss(outputs['shape'], shape_true))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n# ...\nreactivity_model = DualReactivityModel()\ntrain_reactivity_stage()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StructureDataset(Dataset):\n    def __init__(self, seq_csvs, labels_csvs, max_len=512):\n        sequences = []\n        labels = []\n        \n        for seq_csv, label_csv in zip(seq_csvs, labels_csvs):\n            sequences.append(pl.read_csv(seq_csv))\n            labels.append(pl.read_csv(label_csv))\n        \n        self.sequences = pl.concat(sequences)\n        self.labels = pl.concat(labels)\n        \n        # Group by ID to get all residues for each structure\n        self.label_groups = self.labels.group_by('ID').agg([\n            pl.col('resid'),\n            pl.col('resname'),\n            pl.col('x_1'), pl.col('y_1'), pl.col('z_1')\n        ])\n        \n        self.base_to_token = {\n            'A': 0, 'C': 1, 'G': 2, 'U': 3, 'T': 3,\n            'R': 4, 'Y': 5, 'K': 6, 'M': 7, 'S': 8, 'W': 9,\n            'B': 10, 'D': 11, 'H': 12, 'V': 13, 'N': 14, '-': 15\n        }\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        seq_row = self.sequences.row(idx, named=True)\n        target_id = seq_row['target_id']\n        sequence = seq_row['sequence'].upper().replace('T', 'U')\n        \n        # Tokenize\n        tokens = [self.base_to_token.get(base, 14) for base in sequence[:self.max_len]]\n        tokens += [15] * (self.max_len - len(tokens))\n        \n        # Get all residues for this structure\n        coords = np.zeros((self.max_len, 3), dtype=np.float32)\n        \n        structure_data = self.label_groups.filter(pl.col('ID') == target_id)\n        if len(structure_data) > 0:\n            structure_row = structure_data.row(0, named=True)\n            \n            # Sort by resid to ensure correct order\n            resids = structure_row['resid']\n            x_coords = structure_row['x_1']\n            y_coords = structure_row['y_1'] \n            z_coords = structure_row['z_1']\n            \n            # Fill coordinate array\n            for resid, x, y, z in zip(resids, x_coords, y_coords, z_coords):\n                if 0 < resid <= self.max_len:\n                    coords[resid-1] = [x, y, z]\n        \n        # Mask and distance matrix\n        mask = np.zeros(self.max_len, dtype=bool)\n        mask[:min(len(sequence), self.max_len)] = True\n        \n        # Only compute distances for residues with coordinates\n        dist_matrix = np.zeros((self.max_len, self.max_len), dtype=np.float32)\n        for i in range(self.max_len):\n            for j in range(self.max_len):\n                if coords[i].any() and coords[j].any():\n                    dist_matrix[i,j] = np.linalg.norm(coords[i] - coords[j])\n        \n        return {\n            'tokens': torch.LongTensor(tokens),\n            'coords': torch.FloatTensor(coords),  # C1' atoms only\n            'distance_matrix': torch.FloatTensor(dist_matrix),\n            'mask': torch.BoolTensor(mask),\n            'target_id': target_id,\n            'seq_len': len(sequence)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load model\nmodel = RhoFold(rhofold_config)\ncheckpoint = torch.load(\"/kaggle/input/rhofold/pytorch/default/2/RhoFold-main/pretrained/rhofold_pretrained_params.pt\", map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint['model'])\nmodel.eval()\n\n# Test dimensions with dummy data\nseq_len = 100\nbatch_size = 2\n\nprint(\"=== EXPLORING RHOFOLD DIMENSIONS ===\\n\")\n\n# 1. RNA-FM Component\nprint(\"1. RNA-FM (Foundation Model)\")\nrna_fm = model.msa_embedder.rna_fm\nprint(f\"Embedding vocab size: {rna_fm.embed_tokens.num_embeddings}\")\nprint(f\"Embedding dim: {rna_fm.embed_tokens.embedding_dim}\")\n\n# Create dummy RNA sequence tokens\ndummy_tokens = torch.randint(0, 25, (batch_size, seq_len))\nprint(f\"\\nInput tokens shape: {dummy_tokens.shape}\")\n\nwith torch.no_grad():\n    # Test RNA-FM forward\n    rna_fm_out = rna_fm(dummy_tokens, repr_layers=[12])\n    print(f\"RNA-FM output keys: {rna_fm_out.keys()}\")\n    print(f\"RNA-FM logits shape: {rna_fm_out['logits'].shape}\")\n    print(f\"RNA-FM representations shape: {rna_fm_out['representations'][12].shape}\")\n\n# 2. Check MSA embedder expectations\nprint(\"\\n2. MSA Embedder Input Requirements\")\nprint(\"MSA embedder expects:\")\nprint(\"  - tokens: [batch, n_seq, seq_len] for MSA\")\nprint(\"  - rna_fm_tokens: [batch, seq_len] for RNA-FM\")\n\n# 3. Test simplified custom model\nprint(\"\\n=== SIMPLIFIED CUSTOM MODEL ===\")\n\nclass SimplifiedRhoFold(nn.Module):\n    def __init__(self, original_model):\n        super().__init__()\n        # Extract components\n        self.rna_fm = original_model.msa_embedder.rna_fm\n        self.structure_module = original_model.structure_module\n        self.plddt_head = original_model.plddt_head\n        \n        # New layers for RNA-only input\n        self.single_proj = nn.Linear(640, 384)\n        self.pair_proj = nn.Sequential(\n            nn.Linear(1280, 512),\n            nn.ReLU(),\n            nn.Linear(512, 128)\n        )\n        \n        # Reactivity heads\n        self.dms_head = nn.Linear(640, 1)\n        self.shape_head = nn.Linear(640, 1)\n        \n    def forward(self, tokens):\n        # Get RNA-FM representations\n        rna_out = self.rna_fm(tokens, repr_layers=[12])\n        features = rna_out['representations'][12]  # [B, L, 640]\n        \n        # Reactivity predictions\n        dms = self.dms_head(features).squeeze(-1)  # [B, L]\n        shape = self.shape_head(features).squeeze(-1)  # [B, L]\n        \n        # Create single representation for structure module\n        single_rep = self.single_proj(features)  # [B, L, 384]\n        \n        # Create pair representation\n        B, L, D = features.shape\n        feat_i = features.unsqueeze(2).expand(B, L, L, D)\n        feat_j = features.unsqueeze(1).expand(B, L, L, D)\n        pair_feat = torch.cat([feat_i, feat_j], dim=-1)\n        pair_rep = self.pair_proj(pair_feat)  # [B, L, L, 128]\n        \n        # Structure module expects specific format\n        # Let's check what it needs\n        return {\n            'rna_features': features,\n            'single_rep': single_rep,\n            'pair_rep': pair_rep,\n            'dms': dms,\n            'shape': shape,\n            'shapes': {\n                'rna_features': features.shape,\n                'single_rep': single_rep.shape,\n                'pair_rep': pair_rep.shape,\n                'dms': dms.shape,\n                'shape': shape.shape\n            }\n        }\n\n# Test custom model\ncustom_model = SimplifiedRhoFold(model)\nwith torch.no_grad():\n    outputs = custom_model(dummy_tokens)\n    \nprint(\"\\nCustom model output shapes:\")\nfor k, v in outputs['shapes'].items():\n    print(f\"  {k}: {v}\")\n\n# 4. Inspect structure module input requirements\nprint(\"\\n=== STRUCTURE MODULE REQUIREMENTS ===\")\nprint(\"Checking structure module forward signature...\")\n\n# Let's trace through a minimal forward pass\nprint(\"\\nStructure module expects:\")\nprint(\"  - s: single representation [batch, n_res, c_s]\")\nprint(\"  - z: pair representation [batch, n_res, n_res, c_z]\")\nprint(\"  - backbone frames, rotation matrices, etc.\")\n\n# 5. Check coordinate outputs\nprint(\"\\n=== COORDINATE OUTPUT FORMAT ===\")\nprint(\"Structure module outputs:\")\nprint(\"  - Backbone coordinates: [batch, n_res, 3] for C3'\")\nprint(\"  - All atom coordinates: [batch, n_res, n_atoms, 3]\")\nprint(\"  - pLDDT confidence: [batch, n_res]\")\n\n# 6. Training data requirements summary\nprint(\"\\n=== TRAINING DATA REQUIREMENTS ===\")\nprint(\"For RNA-FM fine-tuning on reactivity:\")\nprint(\"  Input:\")\nprint(\"    - RNA sequences as token ids [batch, seq_len]\")\nprint(\"    - Tokens: 0=pad, 1=cls, 2-5=ACGU (likely)\")\nprint(\"  Targets:\")\nprint(\"    - DMS reactivity: [batch, seq_len] float\")\nprint(\"    - SHAPE reactivity: [batch, seq_len] float\")\nprint(\"\\nFor structure prediction:\")\nprint(\"  Input: Same token ids\")\nprint(\"  Targets:\")\nprint(\"    - 3D coordinates: [batch, seq_len, 3]\")\nprint(\"    - Distance matrix: [batch, seq_len, seq_len]\")\nprint(\"    - Secondary structure: [batch, seq_len] (paired/unpaired)\")\n\n# Quick token check\nprint(\"\\n=== TOKEN MAPPING ===\")\nprint(\"Testing token vocabulary...\")\ntest_seq = \"ACGU\"\n# You'll need to check RhoFold's alphabet utils for exact mapping\nprint(\"Check rhofold.utils.alphabet for exact token mapping\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DualReactivityModel(nn.Module):\n    \"\"\"Stage 1: Pretrain RNA-FM on chemical reactivity data\"\"\"\n    def __init__(self, rna_fm, freeze_early_layers=True):\n        super().__init__()\n        self.rna_fm = rna_fm\n        \n        if freeze_early_layers:\n            for i, layer in enumerate(self.rna_fm.layers):\n                if i < 6:  # Freeze first 6/12 layers\n                    for param in layer.parameters():\n                        param.requires_grad = False\n        \n        # Shared projection\n        self.shared_proj = nn.Sequential(\n            nn.Linear(640, 320),\n            nn.ReLU(),\n            nn.LayerNorm(320)\n        )\n        \n        # Task-specific heads\n        self.dms_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n        self.shape_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n    def forward(self, tokens):\n        # Get RNA-FM representations\n        rna_out = self.rna_fm(tokens, repr_layers=[12])\n        features = rna_out['representations'][12]\n        \n        # Shared features\n        shared = self.shared_proj(features)\n        \n        # Predictions\n        dms = self.dms_head(shared).squeeze(-1)\n        shape = self.shape_head(shared).squeeze(-1)\n        \n        return {\n            'dms': dms,\n            'shape': shape,\n            'features': features  # Keep for later use\n        }\n\n\nclass ChemicallyInformedRhoFold(nn.Module):\n    \"\"\"Full model: RNA-FM + Chemical Knowledge + Structure Prediction\"\"\"\n    def __init__(self, rhofold_model, pretrained_rna_fm=None):\n        super().__init__()\n        \n        # Use pretrained RNA-FM if provided, otherwise use RhoFold's\n        if pretrained_rna_fm is not None:\n            self.rna_fm = pretrained_rna_fm\n        else:\n            self.rna_fm = rhofold_model.msa_embedder.rna_fm\n            \n        # Keep reactivity heads for auxiliary loss during structure training\n        self.shared_proj = nn.Sequential(\n            nn.Linear(640, 320),\n            nn.ReLU(),\n            nn.LayerNorm(320)\n        )\n        \n        self.dms_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n        self.shape_head = nn.Sequential(\n            nn.Linear(320, 160),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(160, 1)\n        )\n        \n        # Projection layers for structure module\n        self.to_single = nn.Sequential(\n            nn.Linear(640, 512),\n            nn.ReLU(),\n            nn.LayerNorm(512),\n            nn.Linear(512, 384)\n        )\n        \n        # Pair representation with chemical info integration\n        self.to_pair = nn.Sequential(\n            nn.Linear(1280 + 4, 512),  # +4 for chemical features\n            nn.ReLU(),\n            nn.LayerNorm(512),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128)\n        )\n        \n        # Structure prediction modules from RhoFold\n        self.structure_module = rhofold_model.structure_module\n        self.plddt_head = rhofold_model.plddt_head\n        self.dist_head = rhofold_model.dist_head\n        \n        # Optional: E2Eformer for refining representations\n        self.use_e2eformer = False  # Set True to include\n        if self.use_e2eformer:\n            self.e2eformer = rhofold_model.e2eformer\n        \n    def create_pair_features(self, features, dms=None, shape=None):\n        \"\"\"Create pair features with optional chemical reactivity integration\"\"\"\n        B, L, D = features.shape\n        \n        # Basic pair features from concatenation\n        feat_i = features.unsqueeze(2).expand(B, L, L, D)\n        feat_j = features.unsqueeze(1).expand(B, L, L, D)\n        pair_feat = torch.cat([feat_i, feat_j], dim=-1)  # [B, L, L, 1280]\n        \n        # Add chemical reactivity information if available\n        if dms is not None and shape is not None:\n            # Create reactivity difference features\n            dms_i = dms.unsqueeze(2).expand(B, L, L)\n            dms_j = dms.unsqueeze(1).expand(B, L, L)\n            shape_i = shape.unsqueeze(2).expand(B, L, L)\n            shape_j = shape.unsqueeze(1).expand(B, L, L)\n            \n            # Reactivity differences can indicate pairing\n            dms_diff = (dms_i - dms_j).unsqueeze(-1)\n            dms_prod = (dms_i * dms_j).unsqueeze(-1)\n            shape_diff = (shape_i - shape_j).unsqueeze(-1)\n            shape_prod = (shape_i * shape_j).unsqueeze(-1)\n            \n            chem_features = torch.cat([\n                dms_diff, dms_prod, shape_diff, shape_prod\n            ], dim=-1)  # [B, L, L, 4]\n            \n            pair_feat = torch.cat([pair_feat, chem_features], dim=-1)\n        \n        return self.to_pair(pair_feat)\n    \n    def forward(self, tokens, return_reactivity=True):\n        # RNA-FM encoding\n        rna_out = self.rna_fm(tokens, repr_layers=[12])\n        features = rna_out['representations'][12]  # [B, L, 640]\n        \n        # Predict reactivity (auxiliary task)\n        shared = self.shared_proj(features)\n        dms = self.dms_head(shared).squeeze(-1)\n        shape = self.shape_head(shared).squeeze(-1)\n        \n        # Create structure module inputs\n        single_rep = self.to_single(features)  # [B, L, 384]\n        pair_rep = self.create_pair_features(\n            features, dms.detach(), shape.detach()\n        )  # [B, L, L, 128]\n        \n        # Optional: Run through E2Eformer for refinement\n        if self.use_e2eformer:\n            # Create dummy MSA (single sequence repeated)\n            msa_rep = single_rep.unsqueeze(1).expand(-1, 5, -1, -1)\n            msa_rep, pair_rep = self.e2eformer(msa_rep, pair_rep)\n            single_rep = msa_rep[:, 0]  # Take first sequence\n        \n        # Structure prediction\n        struct_outputs = self.structure_module(\n            single_rep, \n            pair_rep,\n            # Initial coordinates can be provided here if available\n        )\n        \n        # Predict confidence\n        plddt = self.plddt_head(single_rep)\n        \n        # Distance prediction\n        dist = self.dist_head(pair_rep)\n        \n        outputs = {\n            'coordinates': struct_outputs['positions'],  \n            'plddt': plddt,\n            'distogram': dist,\n        }\n        \n        if return_reactivity:\n            outputs.update({\n                'dms': dms,\n                'shape': shape\n            })\n            \n        return outputs\n\n\n# Usage Example:\ndef create_full_model(rhofold_checkpoint_path, reactivity_checkpoint_path=None):\n    \"\"\"Create the full model with optional pretrained reactivity weights\"\"\"\n    \n    # Load base RhoFold\n    from rhofold.config import rhofold_config\n    from rhofold.rhofold import RhoFold\n    \n    rhofold = RhoFold(rhofold_config)\n    checkpoint = torch.load(rhofold_checkpoint_path, map_location='cpu')\n    rhofold.load_state_dict(checkpoint['model'])\n    \n    # Create model\n    if reactivity_checkpoint_path:\n        # Load pretrained reactivity model\n        reactivity_model = DualReactivityModel(rhofold.msa_embedder.rna_fm)\n        reactivity_checkpoint = torch.load(reactivity_checkpoint_path, map_location='cpu')\n        reactivity_model.load_state_dict(reactivity_checkpoint)\n        \n        # Use pretrained RNA-FM in full model\n        model = ChemicallyInformedRhoFold(\n            rhofold, \n            pretrained_rna_fm=reactivity_model.rna_fm\n        )\n        \n        # Also load reactivity heads\n        model.shared_proj.load_state_dict(reactivity_model.shared_proj.state_dict())\n        model.dms_head.load_state_dict(reactivity_model.dms_head.state_dict())\n        model.shape_head.load_state_dict(reactivity_model.shape_head.state_dict())\n    else:\n        # Start from base RhoFold\n        model = ChemicallyInformedRhoFold(rhofold)\n    \n    return model\n\n\n# Training Example:\ndef train_reactivity_stage(model, dataloader, epochs=10):\n    \"\"\"Stage 1: Train on reactivity data\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    for epoch in range(epochs):\n        for batch in dataloader:\n            tokens = batch['tokens']\n            dms_true = batch['dms']\n            shape_true = batch['shape']\n            \n            outputs = model(tokens)\n            \n            # Multi-task loss\n            loss = (F.mse_loss(outputs['dms'], dms_true) + \n                   F.mse_loss(outputs['shape'], shape_true))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n\ndef train_structure_stage(model, dataloader, epochs=10):\n    \"\"\"Stage 2: Train on structure data with auxiliary reactivity loss\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n    \n    for epoch in range(epochs):\n        for batch in dataloader:\n            tokens = batch['tokens']\n            coords_true = batch['coordinates']\n            \n            outputs = model(tokens)\n            \n            # Structure loss\n            coord_loss = F.mse_loss(outputs['coordinates'], coords_true)\n            \n            # Auxiliary reactivity loss if available\n            aux_loss = 0\n            if 'dms' in batch:\n                aux_loss += 0.1 * F.mse_loss(outputs['dms'], batch['dms'])\n                aux_loss += 0.1 * F.mse_loss(outputs['shape'], batch['shape'])\n            \n            loss = coord_loss + aux_loss\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}